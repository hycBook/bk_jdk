<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>Spark集群搭建 · Java相关学习记录</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-change_girls/girls.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="JDBC_JDBCTemplate.html" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"narutohyc","repo":"bk_jdk","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="输入并搜索" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://github.com/narutohyc" target="_blank">我的狗窝</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter" data-level="1.2" data-path="设计模式.html">
<a href="设计模式.html">
<b>1.2.</b>
                    
                    设计模式
            
                </a>
</li>
<li class="chapter" data-level="1.3" data-path="多线程.md">
<span>
<b>1.3.</b>
                    
                    多线程
            
                
            

            
            <ul class="articles">
<li class="chapter" data-level="1.3.1" data-path="多线程_基础篇.html">
<a href="多线程_基础篇.html">
<b>1.3.1.</b>
                    
                    多线程_基础篇
            
                </a>
</li>
<li class="chapter" data-level="1.3.2" data-path="多线程_锁.html">
<a href="多线程_锁.html">
<b>1.3.2.</b>
                    
                    多线程_锁
            
                </a>
</li>
<li class="chapter" data-level="1.3.3" data-path="多线程_JUC原子类.html">
<a href="多线程_JUC原子类.html">
<b>1.3.3.</b>
                    
                    多线程_JUC原子类
            
                </a>
</li>
<li class="chapter" data-level="1.3.4" data-path="多线程_JUC锁集合 .html">
<a href="多线程_JUC锁集合 .html">
<b>1.3.4.</b>
                    
                    多线程_JUC锁集合 
            
                </a>
</li>
<li class="chapter" data-level="1.3.5" data-path="多线程_ JUC集合.html">
<a href="多线程_ JUC集合.html">
<b>1.3.5.</b>
                    
                    多线程_ JUC集合
            
                </a>
</li>
<li class="chapter" data-level="1.3.6" data-path="多线程_JUC线程池.html">
<a href="多线程_JUC线程池.html">
<b>1.3.6.</b>
                    
                    多线程_JUC线程池
            
                </a>
</li>
<li class="chapter" data-level="1.3.7" data-path="多线程_生产者消费者.html">
<a href="多线程_生产者消费者.html">
<b>1.3.7.</b>
                    
                    多线程_生产者消费者
            
                </a>
</li>
</ul>
</span></li>
<li class="chapter" data-level="1.4" data-path="IDEA快捷键.html">
<a href="IDEA快捷键.html">
<b>1.4.</b>
                    
                    IDEA快捷键
            
                </a>
</li>
<li class="chapter" data-level="1.5" data-path="Spring注解.md">
<span>
<b>1.5.</b>
                    
                    Spring注解
            
                
            

            
            <ul class="articles">
<li class="chapter" data-level="1.5.1" data-path="Spring注解_介绍.html">
<a href="Spring注解_介绍.html">
<b>1.5.1.</b>
                    
                    Spring注解_介绍
            
                </a>
</li>
<li class="chapter" data-level="1.5.2" data-path="Spring注解_常用注解.html">
<a href="Spring注解_常用注解.html">
<b>1.5.2.</b>
                    
                    Spring注解_常用注解
            
                </a>
</li>
<li class="chapter" data-level="1.5.3" data-path="Spring注解_Spring入门篇.html">
<a href="Spring注解_Spring入门篇.html">
<b>1.5.3.</b>
                    
                    Spring注解_Spring入门篇
            
                </a>
</li>
</ul>
</span></li>
<li class="chapter" data-level="1.6" data-path="Java基础.md">
<span>
<b>1.6.</b>
                    
                    Java基础
            
                
            

            
            <ul class="articles">
<li class="chapter" data-level="1.6.1" data-path="Java基础_集合.html">
<a href="Java基础_集合.html">
<b>1.6.1.</b>
                    
                    Java基础_集合
            
                </a>
</li>
<li class="chapter" data-level="1.6.2" data-path="Java基础_泛型.html">
<a href="Java基础_泛型.html">
<b>1.6.2.</b>
                    
                    Java基础_泛型
            
                </a>
</li>
<li class="chapter" data-level="1.6.3" data-path="Java基础_打jar包.html">
<a href="Java基础_打jar包.html">
<b>1.6.3.</b>
                    
                    Java基础_打jar包
            
                </a>
</li>
<li class="chapter" data-level="1.6.4" data-path="Java基础_正则匹配.html">
<a href="Java基础_正则匹配.html">
<b>1.6.4.</b>
                    
                    Java基础_正则匹配
            
                </a>
</li>
<li class="chapter" data-level="1.6.5" data-path="Java基础_反射机制.html">
<a href="Java基础_反射机制.html">
<b>1.6.5.</b>
                    
                    Java基础_反射机制
            
                </a>
</li>
<li class="chapter" data-level="1.6.6" data-path="Java基础_对象正反序列化.html">
<a href="Java基础_对象正反序列化.html">
<b>1.6.6.</b>
                    
                    Java基础_对象正反序列化
            
                </a>
</li>
<li class="chapter" data-level="1.6.7" data-path="Java基础_读取配置文件.html">
<a href="Java基础_读取配置文件.html">
<b>1.6.7.</b>
                    
                    Java基础_读取配置文件
            
                </a>
</li>
<li class="chapter" data-level="1.6.8" data-path="Java基础_解析与生成XML.html">
<a href="Java基础_解析与生成XML.html">
<b>1.6.8.</b>
                    
                    Java基础_解析与生成XML
            
                </a>
</li>
<li class="chapter" data-level="1.6.9" data-path="Java基础_后台服务器开发.html">
<a href="Java基础_后台服务器开发.html">
<b>1.6.9.</b>
                    
                    Java基础_后台服务器开发
            
                </a>
</li>
<li class="chapter" data-level="1.6.10" data-path="Java基础_四舍五入.html">
<a href="Java基础_四舍五入.html">
<b>1.6.10.</b>
                    
                    Java基础_四舍五入
            
                </a>
</li>
</ul>
</span></li>
<li class="chapter" data-level="1.7" data-path="Java进阶.md">
<span>
<b>1.7.</b>
                    
                    Java进阶
            
                
            

            
            <ul class="articles">
<li class="chapter" data-level="1.7.1" data-path="Java进阶_反射.html">
<a href="Java进阶_反射.html">
<b>1.7.1.</b>
                    
                    Java进阶_反射
            
                </a>
</li>
<li class="chapter" data-level="1.7.2" data-path="Java进阶_注解.html">
<a href="Java进阶_注解.html">
<b>1.7.2.</b>
                    
                    Java进阶_注解
            
                </a>
</li>
</ul>
</span></li>
<li class="chapter" data-level="1.8" data-path="MySQL.md">
<span>
<b>1.8.</b>
                    
                    MySQL
            
                
            

            
            <ul class="articles">
<li class="chapter" data-level="1.8.1" data-path="MySQL_基础.html">
<a href="MySQL_基础.html">
<b>1.8.1.</b>
                    
                    MySQL_基础
            
                </a>
</li>
<li class="chapter" data-level="1.8.2" data-path="MySQL_约束.html">
<a href="MySQL_约束.html">
<b>1.8.2.</b>
                    
                    MySQL_约束
            
                </a>
</li>
<li class="chapter" data-level="1.8.3" data-path="MySQL_多表查询.html">
<a href="MySQL_多表查询.html">
<b>1.8.3.</b>
                    
                    MySQL_多表查询
            
                </a>
</li>
<li class="chapter" data-level="1.8.4" data-path="MySQL_事务.html">
<a href="MySQL_事务.html">
<b>1.8.4.</b>
                    
                    MySQL_事务
            
                </a>
</li>
<li class="chapter" data-level="1.8.5" data-path="MySQL_数据库设计.html">
<a href="MySQL_数据库设计.html">
<b>1.8.5.</b>
                    
                    MySQL_数据库设计
            
                </a>
</li>
</ul>
</span></li>
<li class="chapter" data-level="1.9" data-path="JDBC.md">
<span>
<b>1.9.</b>
                    
                    JDBC
            
                
            

            
            <ul class="articles">
<li class="chapter" data-level="1.9.1" data-path="JDBC_JDBC基础.html">
<a href="JDBC_JDBC基础.html">
<b>1.9.1.</b>
                    
                    JDBC_JDBC基础
            
                </a>
</li>
<li class="chapter" data-level="1.9.2" data-path="JDBC_JDBC连接池.html">
<a href="JDBC_JDBC连接池.html">
<b>1.9.2.</b>
                    
                    JDBC_JDBC连接池
            
                </a>
</li>
<li class="chapter" data-level="1.9.3" data-path="JDBC_JDBCTemplate.html">
<a href="JDBC_JDBCTemplate.html">
<b>1.9.3.</b>
                    
                    JDBC_JDBCTemplate
            
                </a>
</li>
</ul>
</span></li>
<li class="chapter active" data-level="1.10" data-path="Spark集群搭建.html">
<a href="Spark集群搭建.html">
<b>1.10.</b>
                    
                    Spark集群搭建
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            本书使用 GitBook 发布
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">Spark集群搭建</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#spark环境搭建">1 spark环境搭建</a></li><ul><li><span class="title-icon"></span><a href="#版本总览">1.1 版本总览</a></li><li><span class="title-icon"></span><a href="#scala">1.2 scala</a></li><li><span class="title-icon"></span><a href="#免密登录">1.3 免密登录</a></li><ul><li><span class="title-icon"></span><a href="#修改主机名非必须">1.3.1 修改主机名(非必须)</a></li><li><span class="title-icon"></span><a href="#生成秘钥对">1.3.2 生成秘钥对</a></li><li><span class="title-icon"></span><a href="#秘钥上传服务器">1.3.3 秘钥上传服务器</a></li><li><span class="title-icon"></span><a href="#公钥查看">1.3.4 公钥查看</a></li><li><span class="title-icon"></span><a href="#免密检测">1.3.5 免密检测</a></li></ul><li><span class="title-icon"></span><a href="#hadoop">1.4 hadoop</a></li><ul><li><span class="title-icon"></span><a href="#新建用户非必须">1.4.1 新建用户(非必须)</a></li><li><span class="title-icon"></span><a href="#安装">1.4.2 安装</a></li><li><span class="title-icon"></span><a href="#配置文件">1.4.3 配置文件</a></li><li><span class="title-icon"></span><a href="#从节点设置">1.4.4 从节点设置</a></li><li><span class="title-icon"></span><a href="#hadoop启动和测试">1.4.5 hadoop启动和测试</a></li><li><span class="title-icon"></span><a href="#主要命令">1.4.6 主要命令</a></li><li><span class="title-icon"></span><a href="#常见问题">1.4.7 常见问题</a></li></ul><li><span class="title-icon"></span><a href="#spark安装">1.5 spark安装</a></li><ul><li><span class="title-icon"></span><a href="#安装配置">1.5.1 安装配置</a></li><li><span class="title-icon"></span><a href="#日志配置">1.5.2 日志配置</a></li><li><span class="title-icon"></span><a href="#主要命令_1">1.5.3 主要命令</a></li><li><span class="title-icon"></span><a href="#常见问题_1">1.5.4 常见问题</a></li></ul></ul><li><span class="title-icon"></span><a href="#pyspark">2 pyspark</a></li><ul><li><span class="title-icon"></span><a href="#环境配置">2.1 环境配置</a></li><ul><li><span class="title-icon"></span><a href="#基础环境">2.1.1 基础环境</a></li><li><span class="title-icon"></span><a href="#graphx环境">2.1.2 graphx环境</a></li></ul><li><span class="title-icon"></span><a href="#配置远程解释器">2.2 配置远程解释器</a></li><ul><li><span class="title-icon"></span><a href="#配置连接">2.2.1 配置连接</a></li><li><span class="title-icon"></span><a href="#配置解释器">2.2.2 配置解释器</a></li><li><span class="title-icon"></span><a href="#其它功能">2.2.3 其它功能</a></li></ul><li><span class="title-icon"></span><a href="#测试例子">2.3 测试例子</a></li><li><span class="title-icon"></span><a href="#本地连接">2.4 本地连接</a></li></ul><li><span class="title-icon"></span><a href="#常见问题_2">3 常见问题</a></li></ul></div><a href="#spark环境搭建" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><p><a data-lightbox="7fc4a22e-33ec-4815-ba14-6d84a3d8123c" data-title="img" href="res/other/异世界蕾姆_1.png"><img alt="img" src="res/other/异世界蕾姆_1.png"/></a></p>
<p><li><span class="title-icon"></span><a href="#spark环境搭建">1 spark环境搭建</a></li><ul><li><span class="title-icon"></span><a href="#版本总览">1.1 版本总览</a></li><li><span class="title-icon"></span><a href="#scala">1.2 scala</a></li><li><span class="title-icon"></span><a href="#免密登录">1.3 免密登录</a></li><ul><li><span class="title-icon"></span><a href="#修改主机名非必须">1.3.1 修改主机名(非必须)</a></li><li><span class="title-icon"></span><a href="#生成秘钥对">1.3.2 生成秘钥对</a></li><li><span class="title-icon"></span><a href="#秘钥上传服务器">1.3.3 秘钥上传服务器</a></li><li><span class="title-icon"></span><a href="#公钥查看">1.3.4 公钥查看</a></li><li><span class="title-icon"></span><a href="#免密检测">1.3.5 免密检测</a></li></ul><li><span class="title-icon"></span><a href="#hadoop">1.4 hadoop</a></li><ul><li><span class="title-icon"></span><a href="#新建用户非必须">1.4.1 新建用户(非必须)</a></li><li><span class="title-icon"></span><a href="#安装">1.4.2 安装</a></li><li><span class="title-icon"></span><a href="#配置文件">1.4.3 配置文件</a></li><li><span class="title-icon"></span><a href="#从节点设置">1.4.4 从节点设置</a></li><li><span class="title-icon"></span><a href="#hadoop启动和测试">1.4.5 hadoop启动和测试</a></li><li><span class="title-icon"></span><a href="#主要命令">1.4.6 主要命令</a></li><li><span class="title-icon"></span><a href="#常见问题">1.4.7 常见问题</a></li></ul><li><span class="title-icon"></span><a href="#spark安装">1.5 spark安装</a></li><ul><li><span class="title-icon"></span><a href="#安装配置">1.5.1 安装配置</a></li><li><span class="title-icon"></span><a href="#日志配置">1.5.2 日志配置</a></li><li><span class="title-icon"></span><a href="#主要命令_1">1.5.3 主要命令</a></li><li><span class="title-icon"></span><a href="#常见问题_1">1.5.4 常见问题</a></li></ul></ul><li><span class="title-icon"></span><a href="#pyspark">2 pyspark</a></li><ul><li><span class="title-icon"></span><a href="#环境配置">2.1 环境配置</a></li><ul><li><span class="title-icon"></span><a href="#基础环境">2.1.1 基础环境</a></li><li><span class="title-icon"></span><a href="#graphx环境">2.1.2 graphx环境</a></li></ul><li><span class="title-icon"></span><a href="#配置远程解释器">2.2 配置远程解释器</a></li><ul><li><span class="title-icon"></span><a href="#配置连接">2.2.1 配置连接</a></li><li><span class="title-icon"></span><a href="#配置解释器">2.2.2 配置解释器</a></li><li><span class="title-icon"></span><a href="#其它功能">2.2.3 其它功能</a></li></ul><li><span class="title-icon"></span><a href="#测试例子">2.3 测试例子</a></li><li><span class="title-icon"></span><a href="#本地连接">2.4 本地连接</a></li></ul><li><span class="title-icon"></span><a href="#常见问题_2">3 常见问题</a></li></p>
<h1 id="spark环境搭建">1 spark环境搭建</h1>
<h2 id="版本总览">1.1 版本总览</h2>
<pre><code class="lang-sh">(ray37) [root@Slave03 huangyc]<span class="hljs-comment"># java -version</span>
java version <span class="hljs-string">"1.8.0_281"</span>

(ray37) [root@Slave03 huangyc]<span class="hljs-comment"># hadoop version</span>
Hadoop 3.2.1
Compiled with protoc 2.5.0

(ray37) [root@Slave03 huangyc]<span class="hljs-comment"># scala -version</span>
Scala code runner version 2.12.15 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc.

(ray37) [root@Slave03 huangyc]<span class="hljs-comment"># sh /usr/spark-3.0/bin/spark-shell</span>
Spark context Web UI available at http://Slave03:4040
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  <span class="hljs-string">'_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.3
      /_/  
Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_281)
</span></code></pre>
<h2 id="scala">1.2 scala</h2>
<blockquote>
<p>下载并安装scala</p>
</blockquote>
<ul>
<li>下载 <a href="https://www.scala-lang.org/download/2.12.15.html" target="_blank">scala-2.12</a></li>
<li>安装<code>rpm -ivh scala-2.12.15.rpm</code></li>
<li>查看scala版本<code>scala -version</code></li>
</ul>
<blockquote>
<p>配置环境</p>
</blockquote>
<ul>
<li><p>查询包中<strong>文件安装位置</strong> -l：</p>
<ul>
<li><code>rpm -ql 包名</code></li>
</ul>
</li>
<li><p>编辑<code>/etc/profile</code>，文件末尾添加配置</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 配置scala环境变量</span>
<span class="hljs-built_in">export</span> SCALA_HOME=/usr/share/scala/bin
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$SCALA_HOME</span>
</code></pre>
</li>
</ul>
<p>执行<code>source /etc/profile</code>令其生效</p>
<blockquote>
<p>其他节点同样配置即可</p>
</blockquote>
<h2 id="免密登录">1.3 免密登录</h2>
<h3 id="修改主机名非必须">1.3.1 修改主机名(非必须)</h3>
<blockquote>
<p>查看主机名用<code>hostname</code>
修改主机名<code>/etc/hostname</code>
修改后重启服务器<code>reboot</code></p>
<p>配置host文件<code>vi /etc/hosts</code></p>
</blockquote>
<pre><code class="lang-sh">192.168.123.21  Slave00
192.168.123.23  Slave02
192.168.123.24  Slave03
</code></pre>
<blockquote>
<p><code>/etc/init.d/network restart</code></p>
</blockquote>
<h3 id="生成秘钥对">1.3.2 生成秘钥对</h3>
<blockquote>
<p>使用 <code>RSA</code> 类型的加密类型来创建密钥对</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># 生成秘钥(-f换名字好像不行)</span>
ssh-keygen -t rsa
<span class="hljs-comment"># 配置本机免密</span>
cat id_rsa.pub&gt;&gt;authorized_keys
</code></pre>
<ul>
<li>-f 参数表示指定密钥对生成位置与名称 </li>
<li>密钥对通常放在 <code>~/.ssh</code>目录下 </li>
<li>回车即可创建密钥对，需要输入密码如果不需要为密钥对进行加密，那么可以一路回车</li>
<li>有需要清空文件的可以执行<code>truncate -s 0 authorized_keys</code></li>
</ul>
<p>创建成功之后，可以看到 .ssh 目录下多了两个文件，分别是：</p>
<ol>
<li>id_key：密钥对的<strong>私钥</strong>，通常放在<strong>客户端</strong></li>
<li>id_rsa.pub：密钥对中的<strong>公钥</strong>，通常放在<strong>服务端</strong></li>
</ol>
<h3 id="秘钥上传服务器">1.3.3 秘钥上传服务器</h3>
<blockquote>
<p>文件权限</p>
</blockquote>
<pre><code class="lang-sh">sudo chmod -R 700 ~/.ssh
sudo chmod -R 600 ~/.ssh/authorized_keys
</code></pre>
<blockquote>
<p>将your_key.pub 公钥文件上传至需要连接的服务器</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># 方式一：追加在其他服务器文件末尾(本机不需要)</span>
cat ~/.ssh/id_rsa.pub | ssh root@192.168.123.21 <span class="hljs-string">"cat - &gt;&gt; ~/.ssh/authorized_keys"</span>
cat ~/.ssh/id_rsa.pub | ssh root@192.168.123.23 <span class="hljs-string">"cat - &gt;&gt; ~/.ssh/authorized_keys"</span>
cat ~/.ssh/id_rsa.pub | ssh root@192.168.123.24 <span class="hljs-string">"cat - &gt;&gt; ~/.ssh/authorized_keys"</span>

<span class="hljs-comment"># 方式二</span>
ssh-copy-id -i ~/.ssh/id_key.pub root@192.168.123.21
ssh-copy-id -i ~/.ssh/id_key.pub root@192.168.123.23
ssh-copy-id -i ~/.ssh/id_key.pub root@192.168.123.24
</code></pre>
<ul>
<li>-i 参数表示使用指定的密钥，-p参数表示指定端口，ssh 的默认端口是 22</li>
</ul>
<h3 id="公钥查看">1.3.4 公钥查看</h3>
<p>本地的公钥文件上传在服务器的.ssh/authorized_keys 文件中</p>
<pre><code class="lang-sh">cat ~/.ssh/authorized_keys
</code></pre>
<h3 id="免密检测">1.3.5 免密检测</h3>
<pre><code class="lang-sh">ssh Slaver00
ssh Slaver02
ssh Slaver03
</code></pre>
<h2 id="hadoop">1.4 hadoop</h2>
<h3 id="新建用户非必须">1.4.1 新建用户(非必须)</h3>
<blockquote>
<p>新建hadoop用户</p>
</blockquote>
<pre><code class="lang-sh">useradd -m hadoop <span class="hljs-_">-s</span> /bin/bash
</code></pre>
<blockquote>
<p>修改hadoop用户密码</p>
</blockquote>
<pre><code class="lang-sh">passwd hadoop
</code></pre>
<blockquote>
<p>切换用户</p>
</blockquote>
<pre><code class="lang-sh">su hadoop
</code></pre>
<blockquote>
<p>新建文件夹</p>
</blockquote>
<pre><code class="lang-sh">mkdir /home/hadoop/apps
mkdir /home/hadoop/data
</code></pre>
<blockquote>
<p>关闭防火墙</p>
</blockquote>
<pre><code class="lang-sh">systemctl stop firewalld
systemctl <span class="hljs-built_in">disable</span> firewalld
</code></pre>
<blockquote>
<p>关闭selinux</p>
</blockquote>
<pre><code class="lang-sh">vim /etc/sysconfig/selinux
修改SELINUX=enforcing为SELINUX=disabled
</code></pre>
<h3 id="安装">1.4.2 安装</h3>
<blockquote>
<p>下载并解压hadoop到<code>/usr/hadoop-3.2.0</code></p>
</blockquote>
<ul>
<li><p>下载 <a href="https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz" target="_blank">hadoop-3.2.0</a>，解压到<code>/usr/hadoop-3.2.0</code></p>
</li>
<li><p>配置环境变量</p>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> HADOOP_HOME=/usr/hadoop-3.2.0
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$HADOOP_HOME</span>
</code></pre>
</li>
<li><p>执行<code>source /etc/profile</code>令其生效</p>
</li>
<li><p>查看hadoop版本，<code>hadoop version</code></p>
</li>
</ul>
<h3 id="配置文件">1.4.3 配置文件</h3>
<blockquote>
<p>Hadoop配置文件的配置，需要配置的文件有几个分别是</p>
<ul>
<li>hadoop-env.sh</li>
<li>core-site.xml</li>
<li>hdfs-site.xml</li>
<li>mapred-site.xml</li>
<li>yarn-site.xml</li>
<li>workers文件</li>
</ul>
<p>这些文件均可以在<code>/usr/hadoop-3.2.0/etc/hadoop</code>下找到</p>
</blockquote>
<hr/>
<blockquote>
<p>workers文件</p>
</blockquote>
<pre><code class="lang-sh">Slave00
Slave02
Slave03  <span class="hljs-comment"># 主节点</span>
</code></pre>
<p>旧版本以及网上的教程是修改<code>etc/hadoop/slaves</code>文件，但是新版已经移除了这一个文件，取而代之的是<code>workers</code>文件，上述设置代表我的集群有三个<code>datanode</code>结点</p>
<blockquote>
<p>hadoop_env.sh
在hadoop根目录下执行<code>mkdir pids</code>，用于存放pid文件</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> JAVA_HOME=<span class="hljs-variable">${JAVA_HOME}</span>                    <span class="hljs-comment">#设置JAVA_HOME的路径，需要再次指明</span>
<span class="hljs-built_in">export</span> HADOOP_HOME=<span class="hljs-variable">${HADOOP_HOME}</span>
<span class="hljs-built_in">export</span> HADOOP_PID_DIR=/usr/hadoop-3.2.0/pids     <span class="hljs-comment"># pid文件根目录，不设置的默认值为/tmp，一段时间后/tmp下的文件会被清除，导致无法关闭hadoop集群</span>
<span class="hljs-comment"># 解决启动时警告 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span>
<span class="hljs-built_in">export</span> JAVA_LIBRARY_PATH=<span class="hljs-variable">$HADOOP_HOME</span>/lib/native
</code></pre>
<p>注意的是，如果之前没有设置JAVA_HOME的环境变量，此处直接这样引用会出现错误，改用绝对路径即可消除错误。</p>
<blockquote>
<p>core-site.xml</p>
</blockquote>
<pre><code class="lang-xml"><span class="hljs-comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>
    <span class="hljs-comment">&lt;!-- 设置了主节点的ip --&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://Slave03:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
    <span class="hljs-comment">&lt;!-- 设置了临时目录的地址 --&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/home/huangyc/hadoop_data/data/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span>
</code></pre>
<blockquote>
<p>hdfs-site.xml</p>
</blockquote>
<pre><code class="lang-xml"><span class="hljs-comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>Slave03:50090<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-comment">&lt;!-- 配置网页端口访问 不配置的话 默认端口为9870--&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>Slave03:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:///data/hadoop/hdfs/nn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:///data/hadoop/hdfs/dn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span>
</code></pre>
<blockquote>
<p>mapred-site.xml</p>
</blockquote>
<pre><code class="lang-xml"><span class="hljs-comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>Slave03:10020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>Slave03:19888<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span>
</code></pre>
<blockquote>
<p>yarn-site.xml</p>
</blockquote>
<pre><code class="lang-xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>
  <span class="hljs-comment">&lt;!-- Site specific YARN configuration properties --&gt;</span>
  <span class="hljs-comment">&lt;!-- 指定ResourceManager的地址--&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>Slave03<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
  <span class="hljs-comment">&lt;!-- 指定reducer获取数据的方式--&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:///data/hadoop/yarn/nm<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span>
</code></pre>
<h3 id="从节点设置">1.4.4 从节点设置</h3>
<blockquote>
<p>将hadoop目录拷贝到从节点的对应目录
仿照主节点对应的数据目录设置，设置对应的数据目录（即/data一系列目录）</p>
</blockquote>
<h3 id="hadoop启动和测试">1.4.5 hadoop启动和测试</h3>
<blockquote>
<p>第一次开启时首先要初始化hdfs</p>
</blockquote>
<pre><code class="lang-text">cd /usr/hadoop-3.2.0
./bin/hdfs namenode -format
</code></pre>
<p>此处需要注意一个问题，第二次开启不需要再次初始化，遇到问题需要再次初始化，建议删除存放文件
上述配置文件中指明了存放数据的文件为dfs，到时候删除即可</p>
<blockquote>
<p>主节点启动</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-built_in">cd</span> /usr/hadoop-3.2.0/sbin
<span class="hljs-comment"># 启动前</span>
./start-all.sh
</code></pre>
<blockquote>
<p>查看进程</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># 主节点</span>
[root@Slave03 sbin]<span class="hljs-comment"># jps</span>
28260 SecondaryNameNode
28810 ResourceManager
27706 DataNode
30236 Jps
29341 NodeManager
27326 NameNode

<span class="hljs-comment"># 从节点</span>
[root@Slave00 sbin]<span class="hljs-comment"># jps</span>
3570 DataNode
4115 NodeManager
22413 Worker
27503 Jps
</code></pre>
<blockquote>
<p>查看对应的数据结点是否正确</p>
</blockquote>
<pre><code class="lang-sh">[root@Slave03 sbin]<span class="hljs-comment"># hdfs dfsadmin -report</span>
Configured Capacity: 160982630400 (149.93 GB)
Present Capacity: 82032381952 (76.40 GB)
DFS Remaining: 82032345088 (76.40 GB)
DFS Used: 36864 (36 KB)
DFS Used%: 0.00%
Replicated Blocks:
        Under replicated blocks: 0
        Blocks with corrupt replicas: 0
        Missing blocks: 0
        Missing blocks (with replication factor 1): 0
        Low redundancy blocks with highest priority to recover: 0
        Pending deletion blocks: 0
Erasure Coded Block Groups: 
        Low redundancy block groups: 0
        Block groups with corrupt internal blocks: 0
</code></pre>
<ul>
<li>hdfs主页<code>Slave03:50070</code>，如果不行，试试默认端口<code>9870</code></li>
<li>hadoop主页<code>Slave03:8088</code></li>
</ul>
<h3 id="主要命令">1.4.6 主要命令</h3>
<ul>
<li>启动集群: <code>./sbin/start-all.sh</code></li>
<li>关闭集群: <code>./sbin/stop-all.sh</code></li>
</ul>
<h3 id="常见问题">1.4.7 常见问题</h3>
<blockquote>
<p>stop-all.sh的时候hadoop的相关进程都无法停止</p>
</blockquote>
<p>解决方案: 参考spark的<a href="#stop-all问题">常见问题</a>，Hadoop的pid命名规则：</p>
<pre><code class="lang-sh">pid=<span class="hljs-variable">$HADOOP_PID_DIR</span>/hadoop-<span class="hljs-variable">$HADOOP_IDENT_STRING</span>-<span class="hljs-variable">$command</span>.pid
</code></pre>
<p>因此，这里的pid文件名为:</p>
<ol>
<li>hadoop-root-datanode.pid</li>
<li>hadoop-root-namenode.pid</li>
<li>hadoop-root-nodemanager.pid</li>
<li>hadoop-root-resourcemanager.pid</li>
<li>hadoop-root-secondarynamenode.pid</li>
</ol>
<p>通过jps查看相关进程的pid，恢复这些pid文件即可使用stop-all.sh停止hadoop，根治方案参考spark常见问题部分</p>
<h2 id="spark安装">1.5 spark安装</h2>
<h3 id="安装配置">1.5.1 安装配置</h3>
<blockquote>
<p>下载解压，复制到<code>/usr/spark-3.0</code></p>
<p>配置环境变量<code>vi /etc/profile</code></p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># 配置spark环境</span>
<span class="hljs-built_in">export</span> SPARK_HOME=/usr/spark-3.0
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$SPARK_HOME</span>/bin
</code></pre>
<p>执行<code>source /etc/profile</code>令其生效</p>
<blockquote>
<p>配置<code>spark-env.sh</code>
将<code>conf</code>文件夹下的<code>spark-env.sh.template</code>重命名为<code>spark-env.sh</code>，并添加以下内容：
在spark根目录下执行<code>mkdir pids</code>，用于存放pid文件</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># 环境变量</span>
<span class="hljs-built_in">export</span> JAVA_HOME=/usr/java/jdk1.8.0_281-amd64
<span class="hljs-built_in">export</span> SCALA_HOME=/usr/share/scala
<span class="hljs-built_in">export</span> HADOOP_HOME=/usr/hadoop-3.2.1
<span class="hljs-comment"># 详细配置</span>
<span class="hljs-built_in">export</span> HADOOP_CONF_DIR=<span class="hljs-variable">$HADOOP_HOME</span>/etc/hadoop
<span class="hljs-built_in">export</span> SPARK_MASTER_HOST=Slave03
<span class="hljs-built_in">export</span> SPARK_LOCAL_DIRS=/usr/spark-3.0
<span class="hljs-built_in">export</span> SPARK_DRIVER_MEMORY=16g           <span class="hljs-comment"># 内存</span>
<span class="hljs-built_in">export</span> SPARK_EXECUTOR_MEMORY=8g          <span class="hljs-comment"># 执行内存</span>
<span class="hljs-built_in">export</span> SPARK_WORKER_CORES=4              <span class="hljs-comment"># cpu核心数</span>
<span class="hljs-built_in">export</span> SPARK_PID_DIR=/usr/spark-3.0/pids <span class="hljs-comment"># pid文件根目录，不设置的默认值为/tmp，一段时间后/tmp下的文件会被清除，导致无法关闭spark集群</span>
</code></pre>
<blockquote>
<p>配置<code>slaves</code>
将<code>conf</code>文件夹下的<code>slaves.template</code>重命名为<code>slaves</code>，并添加以下内容：</p>
</blockquote>
<pre><code class="lang-sh">Slave00
Slave02
Slave03  <span class="hljs-comment"># 主节点</span>
</code></pre>
<blockquote>
<p>配置从节点(将spark目录复制到其他节点的同一个目录下)</p>
</blockquote>
<pre><code class="lang-sh">scp -r root@192.168.123.24:/usr/spark-3.0 /usr/
</code></pre>
<blockquote>
<p>在<code>sbin</code>目录下使用<code>start-all.sh</code>启动集群</p>
</blockquote>
<p><a data-lightbox="c02e4bcd-5696-44f9-818c-0136f4d4508c" data-title="image-20220125113759221" href="res/Spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20220125113759221.png"><img alt="image-20220125113759221" src="res/Spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20220125113759221.png"/></a></p>
<p>启动成功后，我们在浏览器输入<code>Slave03:8080</code>看到有三个结点，就代表我们安装成功了。
如果发现启动错误，请查看<code>logs</code>目录下的日志，自行检查配置文件！</p>
<h3 id="日志配置">1.5.2 日志配置</h3>
<blockquote>
<p>配置spark的执行日志等级，进入到spark根目录下的<code>conf</code>目录
<code>cp log4j.properties.template log4j.properties</code>，修改配置如下</p>
</blockquote>
<pre><code class="lang-properties"># Set everything to be logged to the console
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Set the default spark-shell log level to WARN. When running the spark-shell, the
# log level for this class is used to overwrite the root logger's log level, so that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=WARN

# Settings to quiet third party logs that are too verbose
log4j.logger.org.sparkproject.jetty=WARN
log4j.logger.org.sparkproject.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
# 关闭警告
# WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR
</code></pre>
<h3 id="主要命令_1">1.5.3 主要命令</h3>
<ul>
<li>启动集群: <code>./sbin/start-all.sh</code></li>
<li>关闭集群: <code>./sbin/stop-all.sh</code></li>
</ul>
<h3 id="常见问题_1">1.5.4 常见问题</h3>
<blockquote>
<p><span id="stop-all问题">stop-all.sh的时候spark的相关进程都无法停止</span></p>
</blockquote>
<p><code>$SPARK_PID_DIR</code>中存放的是pid文件，就是要停止进程的pid。其中$SPARK_PID_DIR默认是在系统的/tmp目录
系统每隔一段时间就会清除/tmp目录下的内容。到/tmp下查看一下，果然没有相关进程的pid文件了。
这才导致了stop-all.sh无法停止集群。
解决方案: <code>$SPARK_PID_DIR</code>下新建<code>pid文件</code>，pid文件命名规则如下</p>
<pre><code class="lang-sh"><span class="hljs-variable">$SPARK_PID_DIR</span>/spark-<span class="hljs-variable">$SPARK_IDENT_STRING</span>-<span class="hljs-variable">$command</span>-<span class="hljs-variable">$instance</span>.pid
</code></pre>
<ul>
<li><code>$SPARK_PID_DIR</code>默认是<code>/tmp</code></li>
<li><code>$SPARK_IDENT_STRING</code>是登录用户<code>$USER</code>，我的集群中用户名是root</li>
<li><code>$command</code>是调用spark-daemon.sh时的参数，有两个：<ol>
<li>org.apache.spark.deploy.master.Master</li>
<li>org.apache.spark.deploy.worker.Worker</li>
</ol>
</li>
<li><code>$instance</code>也是调用spark-daemon.sh时的参数，我的集群中是1</li>
</ul>
<p>因此pid文件名如下(名字不对的情况下，可以执行<code>./start-all.sh</code>，重新启动查看后，再执行<code>./stop-all.sh</code>进行本次集群的关闭，注意这里关闭的是本次打开的，之前无法关闭的进程仍然还在)：</p>
<ol>
<li>spark-root-org.apache.spark.deploy.master.Master-1.pid</li>
<li>spark-root-org.apache.spark.deploy.worker.Worker-1.pid</li>
</ol>
<p>通过jps查看相关进程的pid，将pid保存到对应的pid文件即可，之后调用spark的stop-all.sh，即可正常停止spark集群
要<strong>根治</strong>这个问题，只需要在集群所有节点都设置<code>$SPARK_PID_DIR</code>，<code>$HADOOP_PID_DIR</code>和<code>$YARN_PID_DIR</code>即可</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 修改hadoop-env.sh，增加：</span>
<span class="hljs-built_in">export</span> HADOOP_PID_DIR=/home/ap/cdahdp/app/pids
<span class="hljs-comment"># 修改yarn-env.sh，增加：</span>
<span class="hljs-built_in">export</span> YARN_PID_DIR=/home/ap/cdahdp/app/pids
<span class="hljs-comment"># 修改spark-env.sh，增加：</span>
<span class="hljs-built_in">export</span> SPARK_PID_DIR=/home/ap/cdahdp/app/pids
</code></pre>
<h1 id="pyspark">2 pyspark</h1>
<h2 id="环境配置">2.1 环境配置</h2>
<h3 id="基础环境">2.1.1 基础环境</h3>
<blockquote>
<p>spark集群服务器配置环境变量，<code>vi /etc/profile</code></p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> PYSPARK_PYTHON=/root/anaconda3/envs/ray37/bin/python3.7
<span class="hljs-built_in">export</span> PYSPARK_DRIVER_PYTHON=/root/anaconda3/envs/ray37/bin/python3.7
</code></pre>
<p>执行<code>source /etc/profile</code>使其生效</p>
<blockquote>
<p>安装pyspark库</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># 这里需要注意版本需要和spark的版本一致</span>
pip install pyspark==3.0.3
<span class="hljs-comment"># 在安装pyspark时默认会装上对应的py4j，如果没有的话，手动安装下</span>
pip install py4j==0.10.9
</code></pre>
<blockquote>
<p>对于spark服务器和work间环境不一致的情况</p>
</blockquote>
<ul>
<li><p><strong>方式一</strong>: 重新安装python虚拟环境，使得路径完全一致</p>
</li>
<li><p><strong>方式二</strong>: 配置软链接</p>
<pre><code class="lang-sh">ln <span class="hljs-_">-s</span> 源文件 目标文件
<span class="hljs-comment"># 需要事先建好路径</span>
ln <span class="hljs-_">-s</span> /opt/anaconda/install/envs/ray37/bin/python3.7 /root/anaconda3/envs/ray37/bin/python3.7
</code></pre>
</li>
</ul>
<blockquote>
<p>在服务器 liunx 环境上修改查看python的包路径site-package</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> distutils.sysconfig <span class="hljs-keyword">import</span> get_python_lib
print(get_python_lib())
</code></pre>
<h3 id="graphx环境">2.1.2 graphx环境</h3>
<blockquote>
<p>安装graphframes</p>
</blockquote>
<pre><code class="lang-sh">pip install graphframes==0.6
</code></pre>
<p>去<a href="https://spark-packages.org/package/graphframes/graphframes" target="_blank">graphframes官网</a>下载对应jar包，这里spark集群是3.0.3，所以可以下载<strong>Version: 0.8.2-spark3.0-s_2.12</strong>这个版本的
复制到<code>spark根目录</code>下的<code>jars目录</code>中，集群中每个节点都需要
如果还会出错，可以把该jar包也复制一份到python环境下
<code>/opt/anaconda/install/envs/ray37/lib/python3.7/site-packages/pyspark/jars</code></p>
<h2 id="配置远程解释器">2.2 配置远程解释器</h2>
<h3 id="配置连接">2.2.1 配置连接</h3>
<blockquote>
<p>配置连接的远程服务器</p>
</blockquote>
<p>打开<code>Pycharm -&gt; Tools -&gt; Deployment -&gt; Configuration</code></p>
<blockquote>
<p>点击加号，选择SFTP，给服务器取一个名字</p>
<p>点击SSH configuration后面的省略号
输入<code>服务器IP地址</code>和<code>账号密码</code>，结束后测试一下连接情况
<code>Connection Parameters</code>下的心跳可以设置10秒
确认后，保存退出当前窗口</p>
<p>点击自动检测 ，选择远程服务器的工作路径
这里选的是，自己提前新建的空文件夹<code>/home/huangyc/hyc_test</code></p>
<p><code>Mappings</code>下选择<code>Local path</code>，配置当前项目的路径
保存，退出</p>
</blockquote>
<h3 id="配置解释器">2.2.2 配置解释器</h3>
<blockquote>
<p>选择远程的解释器</p>
</blockquote>
<center><a data-lightbox="6e9b5570-13d5-4d53-b732-5d2310be5c61" data-title="undefined" href="res/Spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20220127143544475.png"><img alt="undefined" src="res/Spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20220127143544475.png"/></a></center>
<blockquote>
<p>点击move即可next
配置路径，远程的选择自己之前新建的<code>/home/huangyc/hyc_test</code>
<code>Automatically upload ...</code>表示会自动上传项目到服务器中</p>
</blockquote>
<center><a data-lightbox="4df44281-1d44-4f34-bb73-2f292072f397" data-title="undefined" href="res/Spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20220127143940559.png"><img alt="undefined" src="res/Spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/image-20220127143940559.png"/></a></center>
<h3 id="其它功能">2.2.3 其它功能</h3>
<ul>
<li>在<code>Pycharm</code>上显示远程代码：选择<code>Tools --&gt;Deployment--&gt;Browse Remote Host</code></li>
<li>更新代码：将本地代码上传到服务器上<code>Tools --&gt;Deployment--&gt;upload to</code></li>
<li>服务器上代码下载到本地代码上<code>Tools --&gt;Deployment--&gt;Download from</code></li>
</ul>
<h2 id="测试例子">2.3 测试例子</h2>
<blockquote>
<p>简单词统计 + pagerank例子</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment">#!/usr/bin/env Python</span>
<span class="hljs-comment"># -- coding: utf-8 --</span>

<span class="hljs-string">"""
@version: v1.0
@author: huangyc
@file: connenct_test.py
@Description: 
@time: 2022/1/25 14:49
"""</span>
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> platform
<span class="hljs-keyword">import</span> traceback
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> List

<span class="hljs-keyword">from</span> graphframes <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> pyspark <span class="hljs-keyword">import</span> SparkConf
<span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSession
<span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> functions <span class="hljs-keyword">as</span> F

py_root = <span class="hljs-string">"/root/anaconda3/envs/ray37/bin/python3.7"</span>
os.environ[<span class="hljs-string">"PYSPARK_PYTHON"</span>] = py_root
os.environ[<span class="hljs-string">"PYSPARK_DRIVER_PYTHON"</span>] = py_root

is_local_py: bool = <span class="hljs-keyword">True</span>
<span class="hljs-keyword">if</span> is_local_py <span class="hljs-keyword">and</span> platform.system().lower() == <span class="hljs-string">'windows'</span>:
    os.environ[<span class="hljs-string">'JAVA_HOME'</span>] = <span class="hljs-string">'G:\Java\jdk1.8.0_201'</span>
    os.environ[<span class="hljs-string">"SPARK_HOME"</span>] = <span class="hljs-string">r"E:\PycharmWS\remote_spark\spark-3.0"</span>


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PySparkClient</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, appname: str, master: str)</span>:</span>
        conf = SparkConf().setAppName(appname).setMaster(master)  <span class="hljs-comment"># spark资源配置</span>
        conf.set(<span class="hljs-string">"spark.driver.maxResultSize"</span>, <span class="hljs-string">"4g"</span>)
        conf.set(<span class="hljs-string">"spark.executor.num"</span>, <span class="hljs-string">"4"</span>)
        conf.set(<span class="hljs-string">"spark.executor.memory"</span>, <span class="hljs-string">"2g"</span>)
        conf.set(<span class="hljs-string">"spark.executor.cores"</span>, <span class="hljs-string">"4"</span>)
        conf.set(<span class="hljs-string">"spark.cores.max"</span>, <span class="hljs-string">"16"</span>)
        conf.set(<span class="hljs-string">"spark.driver.memory"</span>, <span class="hljs-string">"2g"</span>)

        <span class="hljs-keyword">try</span>:
            self.spark = SparkSession.builder.config(conf=conf).getOrCreate()

            self.sc = self.spark.sparkContext
        <span class="hljs-keyword">except</span>:
            traceback.print_exc()  <span class="hljs-comment"># 返回出错信息</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">word_count</span><span class="hljs-params">(self, log_file: str)</span>:</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.sc:
            <span class="hljs-keyword">return</span>

        log_data = self.sc.textFile(log_file).cache()
        words_rdd = log_data.flatMap(<span class="hljs-keyword">lambda</span> sentence: sentence.split(<span class="hljs-string">" "</span>))
        res = words_rdd.countByValue()
        res_rdd = words_rdd.filter(<span class="hljs-keyword">lambda</span> k: len(k) &gt; <span class="hljs-number">0</span>).map(<span class="hljs-keyword">lambda</span> word: (word, <span class="hljs-number">1</span>)).reduceByKey(<span class="hljs-keyword">lambda</span> a, b: a + b)

        <span class="hljs-comment"># 将rdd转为collection并打印</span>
        res_rdd_coll = res_rdd.takeOrdered(<span class="hljs-number">5</span>, <span class="hljs-keyword">lambda</span> x: -x[<span class="hljs-number">1</span>])
        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> res_rdd_coll:
            print(line)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">simple_test</span><span class="hljs-params">(self, words: List[str])</span>:</span>
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.sc:
            <span class="hljs-keyword">return</span>

        counts = self.sc.parallelize(words).count()
        print(<span class="hljs-string">"Number of elements in RDD is %i"</span> % counts)

        test = self.spark.createDataFrame(
            [(<span class="hljs-string">'001'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-number">100</span>, <span class="hljs-number">87</span>, <span class="hljs-number">67</span>, <span class="hljs-number">83</span>, <span class="hljs-number">98</span>), (<span class="hljs-string">'002'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-number">87</span>, <span class="hljs-number">81</span>, <span class="hljs-number">90</span>, <span class="hljs-number">83</span>, <span class="hljs-number">83</span>), (<span class="hljs-string">'003'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-number">86</span>, <span class="hljs-number">91</span>, <span class="hljs-number">83</span>, <span class="hljs-number">89</span>, <span class="hljs-number">63</span>),
             (<span class="hljs-string">'004'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-number">65</span>, <span class="hljs-number">87</span>, <span class="hljs-number">94</span>, <span class="hljs-number">73</span>, <span class="hljs-number">88</span>), (<span class="hljs-string">'005'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-number">76</span>, <span class="hljs-number">62</span>, <span class="hljs-number">89</span>, <span class="hljs-number">81</span>, <span class="hljs-number">98</span>), (<span class="hljs-string">'006'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-number">84</span>, <span class="hljs-number">82</span>, <span class="hljs-number">85</span>, <span class="hljs-number">73</span>, <span class="hljs-number">99</span>),
             (<span class="hljs-string">'007'</span>, <span class="hljs-string">'3'</span>, <span class="hljs-number">56</span>, <span class="hljs-number">76</span>, <span class="hljs-number">63</span>, <span class="hljs-number">72</span>, <span class="hljs-number">87</span>), (<span class="hljs-string">'008'</span>, <span class="hljs-string">'1'</span>, <span class="hljs-number">55</span>, <span class="hljs-number">62</span>, <span class="hljs-number">46</span>, <span class="hljs-number">78</span>, <span class="hljs-number">71</span>), (<span class="hljs-string">'009'</span>, <span class="hljs-string">'2'</span>, <span class="hljs-number">63</span>, <span class="hljs-number">72</span>, <span class="hljs-number">87</span>, <span class="hljs-number">98</span>, <span class="hljs-number">64</span>)],
            [<span class="hljs-string">'number'</span>, <span class="hljs-string">'class'</span>, <span class="hljs-string">'language'</span>, <span class="hljs-string">'math'</span>, <span class="hljs-string">'english'</span>, <span class="hljs-string">'physic'</span>, <span class="hljs-string">'chemical'</span>])
        test.show()
        test.printSchema()

        test.select(<span class="hljs-string">'number'</span>, <span class="hljs-string">'class'</span>, <span class="hljs-string">'language'</span>, <span class="hljs-string">'math'</span>, <span class="hljs-string">'english'</span>).describe().show()

        print(<span class="hljs-string">"============   simple_test over   ============"</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">simple_graph</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-comment"># Create a Vertex DataFrame with unique ID column "id"</span>
        spk = self.spark
        v = spk.createDataFrame([
            (<span class="hljs-string">"a"</span>, <span class="hljs-string">"Alice"</span>, <span class="hljs-number">34</span>),
            (<span class="hljs-string">"b"</span>, <span class="hljs-string">"Bob"</span>, <span class="hljs-number">36</span>),
            (<span class="hljs-string">"c"</span>, <span class="hljs-string">"Charlie"</span>, <span class="hljs-number">30</span>),
        ], [<span class="hljs-string">"id"</span>, <span class="hljs-string">"name"</span>, <span class="hljs-string">"age"</span>])
        <span class="hljs-comment"># Create an Edge DataFrame with "src" and "dst" columns</span>
        e = spk.createDataFrame([
            (<span class="hljs-string">"a"</span>, <span class="hljs-string">"b"</span>, <span class="hljs-string">"friend"</span>),
            (<span class="hljs-string">"b"</span>, <span class="hljs-string">"c"</span>, <span class="hljs-string">"follow"</span>),
            (<span class="hljs-string">"c"</span>, <span class="hljs-string">"b"</span>, <span class="hljs-string">"follow"</span>),
        ], [<span class="hljs-string">"src"</span>, <span class="hljs-string">"dst"</span>, <span class="hljs-string">"relationship"</span>])
        <span class="hljs-comment"># Create a GraphFrame</span>
        g = GraphFrame(v, e)

        <span class="hljs-comment"># Query: Get in-degree of each vertex.</span>
        g.inDegrees.show()

        <span class="hljs-comment"># Query: Count the number of "follow" connections in the graph.</span>
        print(g.edges.filter(<span class="hljs-string">"relationship = 'follow'"</span>).count())

        <span class="hljs-comment"># Run PageRank algorithm, and show results.</span>
        results = g.pageRank(resetProbability=<span class="hljs-number">0.1</span>, maxIter=<span class="hljs-number">1</span>)
        res = results.vertices.select(<span class="hljs-string">"id"</span>, F.bround(<span class="hljs-string">"pagerank"</span>, scale=<span class="hljs-number">4</span>).alias(<span class="hljs-string">'pagerank'</span>))
        res.orderBy(res.pagerank.desc()).show(<span class="hljs-number">5</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">stop</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">try</span>:
            self.sc.stop()
        <span class="hljs-keyword">except</span>:
            <span class="hljs-keyword">pass</span>

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    appname = <span class="hljs-string">"test_hyc00"</span>  <span class="hljs-comment"># 任务名称</span>
    master = <span class="hljs-string">"spark://192.168.xx.xx:7077"</span>  <span class="hljs-comment"># 单机模式设置</span>
    <span class="hljs-comment"># master = "local"</span>
    py_spark_client = PySparkClient(appname=appname, master=master)

    <span class="hljs-comment"># 简单功能测试</span>
    all_words = [<span class="hljs-string">"scala"</span>, <span class="hljs-string">"java"</span>, <span class="hljs-string">"hadoop"</span>, <span class="hljs-string">"spark"</span>, <span class="hljs-string">"akka"</span>, <span class="hljs-string">"spark vs hadoop"</span>,
                 <span class="hljs-string">"akka"</span>, <span class="hljs-string">"spark vs hadoop"</span>, <span class="hljs-string">"pyspark"</span>, <span class="hljs-string">"pyspark and spark"</span>]
    py_spark_client.simple_test(words=all_words)

    <span class="hljs-comment"># 文档词频统计</span>
    logFile = <span class="hljs-string">r"hdfs://192.168.xx.xx:9000/test_data/README.md"</span>
    py_spark_client.word_count(log_file=logFile)

    py_spark_client.simple_graph()

    <span class="hljs-comment"># 关闭客户端连接</span>
    py_spark_client.stop()
</code></pre>
<blockquote>
<p>执行方法</p>
</blockquote>
<ul>
<li><p>可以直接在pycharm中执行，这里pycharm使用的是远程的python环境，这里执行时的内存是<strong>默认值</strong> 1024.0 MiB</p>
<p>在pycharm中执行可能需要加上变量设置，但使用<code>spark-submit</code>则可以不需要</p>
<pre><code class="lang-python">os.environ[<span class="hljs-string">"PYSPARK_PYTHON"</span>] = <span class="hljs-string">"/root/anaconda3/envs/ray37/bin/python3.7"</span>
os.environ[<span class="hljs-string">"PYSPARK_DRIVER_PYTHON"</span>] = <span class="hljs-string">"/root/anaconda3/envs/ray37/bin/python3.7"</span>
</code></pre>
</li>
<li><p>或者执行以下命令执行，这里执行时的内存是spark中<strong>配置</strong>的 8.0 GiB</p>
<pre><code class="lang-sh">./bin/spark-submit  --master spark://192.168.xx.xx:7077 /home/huangyc/hyc_<span class="hljs-built_in">test</span>/tests/pyspark_<span class="hljs-built_in">test</span>/connenct_test.py
</code></pre>
</li>
</ul>
<h2 id="本地连接">2.4 本地连接</h2>
<blockquote>
<p>除了配置远程服务器的py解释器，还可以配置本地的模式</p>
<p>需要在spark集群中配置host映射，通过<code>vi /etc/hosts</code>命令打开hosts文件，添加本地的主机名映射</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># hyc</span>
10.10.0.xx DSE-20191111ZOU
</code></pre>
<p>最后执行<code>/etc/init.d/network restart</code>刷新DNS
如果没有的话，可能会报错误
<code>Caused by: java.io.IOException: Failed to connect to DSE-20191111ZOU:65320</code>
<code>Caused by: java.net.UnknownHostException: DSE-20191111ZOU</code></p>
<blockquote>
<p>py代码中需要添加以下的环境参数才能连接上spark集群</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> os

<span class="hljs-comment"># 配置集群的python环境路径</span>
py_root = <span class="hljs-string">"/root/anaconda3/envs/ray37/bin/python3.7"</span>
os.environ[<span class="hljs-string">"PYSPARK_PYTHON"</span>] = py_root
os.environ[<span class="hljs-string">"PYSPARK_DRIVER_PYTHON"</span>] = py_root
<span class="hljs-comment"># 配置本地Jdk路径，版本尽量和spark集群的一致</span>
os.environ[<span class="hljs-string">'JAVA_HOME'</span>] = <span class="hljs-string">'G:\Java\jdk1.8.0_201'</span>
<span class="hljs-comment"># 配置本地的spark路径，这里直接从spark集群copy一份下载即可</span>
os.environ[<span class="hljs-string">"SPARK_HOME"</span>] = <span class="hljs-string">r"E:\PycharmWS\remote_spark\spark-3.0"</span>

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PySparkClient</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, appname: str, master: str)</span>:</span>
        conf = SparkConf().setAppName(appname).setMaster(master)  <span class="hljs-comment"># spark资源配置</span>
        <span class="hljs-comment"># 以下的参数不用全部配置，看自己的需求</span>
        conf.set(<span class="hljs-string">"spark.driver.maxResultSize"</span>, <span class="hljs-string">"4g"</span>)
        conf.set(<span class="hljs-string">"spark.executor.num"</span>, <span class="hljs-string">"4"</span>)
        conf.set(<span class="hljs-string">"spark.executor.memory"</span>, <span class="hljs-string">"2g"</span>)
        conf.set(<span class="hljs-string">"spark.executor.cores"</span>, <span class="hljs-string">"4"</span>)
        conf.set(<span class="hljs-string">"spark.cores.max"</span>, <span class="hljs-string">"16"</span>)
        conf.set(<span class="hljs-string">"spark.driver.memory"</span>, <span class="hljs-string">"2g"</span>)

        <span class="hljs-keyword">try</span>:
            self.spark = SparkSession.builder.config(conf=conf).getOrCreate()
            self.sc = self.spark.sparkContext
        <span class="hljs-keyword">except</span>:
            traceback.print_exc()  <span class="hljs-comment"># 返回出错信息</span>
</code></pre>
<p>⚡: 本地的<strong>python版本</strong>尽量和spark集群上的一致，避免不必要的错误</p>
<h1 id="常见问题_2">3 常见问题</h1>
<p></p><footer class="page-footer"><span class="copyright">Copyright © narutohyc.com 2022 all right reserved，powered by Gitbook</span><span class="footer-modification">该文件修订时间：
2022-02-16 06:01:05
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: JDBC_JDBCTemplate" class="navigation navigation-prev navigation-unique" href="JDBC_JDBCTemplate.html">
<i class="fa fa-angle-left"></i>
</a>
</div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Spark集群搭建","level":"1.10","depth":1,"previous":{"title":"JDBC_JDBCTemplate","level":"1.9.3","depth":2,"path":"chapters/JDBC_JDBCTemplate.md","ref":"chapters/JDBC_JDBCTemplate.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","favicon","search-plus","-lunr","-search","lightbox","change_girls","theme-comscore","valine","pageview-count","favicon-absolute"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright © narutohyc.com 2022","modify_label":"该文件修订时间：","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/narutohyc"},"splitter":{},"change_girls":{"time":10,"urls":["https://plc.jj20.com/up/allimg/1115/012122143136/220121143136-2.jpg","https://plc.jj20.com/up/allimg/1115/111R1094405/21111P94405-1.jpg"]},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://i.loli.net/2021/01/12/Cdunm9AoBcHF5MI.png","alipayText":"支付宝打赏","button":"赏","title":"","wechat":"https://i.loli.net/2021/01/12/gmzASfCciIFXTyr.png","wechatText":"微信打赏"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"favicon":{},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"narutohyc","repo":"bk_jdk","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"","lang":"zh-CN","pageSize":10,"placeholder":"Just go go","recordIP":false,"appId":"jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz","appKey":"FOXMptWOHC7cU1FxXt0LJj4o"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Java相关学习记录","language":"zh-hans","links":{"sidebar":{"我的狗窝":"https://github.com/narutohyc"}},"gitbook":"*","description":"记录 Java 的学习和一些技巧的使用"},"file":{"path":"chapters/Spark集群搭建.md","mtime":"2022-02-16T06:01:05.687Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2022-02-16T06:02:40.640Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-change_girls/girls.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
